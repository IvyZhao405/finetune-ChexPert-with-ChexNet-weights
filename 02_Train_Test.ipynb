{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version is  2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version is \", tf.__version__)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "keras=tf.keras\n",
    "from src import input_fn\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tensorflow.keras.models import load_model\n",
    "from src import config as cfg\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from src.visualization_utils import plot_auroc_curve,plot_confusion_matrix_custom,plot_precision_recall_curve\n",
    "import efficientnet.tfkeras as efn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "batch_size = cfg.chexper_params['batch_size']\n",
    "lr =  cfg.chexper_params['lr']\n",
    "epoches = cfg.chexper_params['epoches']\n",
    "image_size = cfg.chexper_params['image_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input files\n",
    "train = cfg.input_file['train']\n",
    "test = cfg.input_file['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> select labeling strategy here</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeling approach \n",
    "approach= 'u-Ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load path\n",
    "model_dir = cfg.output_path[approach]['directory']\n",
    "checkpoint_path = cfg.output_path[approach]['checkpoint_path']\n",
    "model_name= cfg.output_path[approach]['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/uIgnore.h5\n"
     ]
    }
   ],
   "source": [
    "model_path= os.path.join('output',model_name)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use previous checkpoint \n",
    "use_checkpoint=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**finetune or transfer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune = cfg.train_approach['finetune']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use ChexNet weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexnet = cfg.train_approach['chexnet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**use EfficientNet** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet = cfg.train_approach['efficientnet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178731\n",
      "178731\n",
      "44683\n",
      "44683\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "train_x,validation_x, train_y,validation_y=input_fn.process_training_data(file_path=train, approach=approach)\n",
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "print(len(validation_x))\n",
    "print(len(validation_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atelectasis,Cardiomegaly,Cosolidation,Edema,Pleural Effusion  \n",
    "new_trainy=train_y[:,[8,2,6,5,10]]\n",
    "new_validationy=validation_y[:,[8,2,6,5,10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load tf.dataset format inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input= input_fn.input_fn_multi_output(True,train_x, new_trainy,batch_size)\n",
    "eval_input= input_fn.input_fn_multi_output(False,validation_x,new_validationy,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##custom accuracy for u-ignore to ignore y=-1 \n",
    "\n",
    "def binary_accuracy(y_true, y_pred):\n",
    "    t0 = tf.equal(y_true, 0)\n",
    "    t1 = tf.equal(y_true, 1)\n",
    "    p0 = tf.equal(tf.round(y_pred), 0)\n",
    "    p1 = tf.equal(tf.round(y_pred), 1)\n",
    "    everything = tf.reduce_sum(tf.cast(t0, tf.int32)) + tf.reduce_sum(tf.cast(t1, tf.int32))\n",
    "    positives = tf.reduce_sum(tf.cast(tf.logical_and(t0, p0), tf.int32)) + tf.reduce_sum(tf.cast(tf.logical_and(p1, t1), tf.int32))\n",
    "    return positives / everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom weighted loss for u-ignore\n",
    "\n",
    "def create_mask_weighted_binary_crossentropy(zero_weight, one_weight):\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "\n",
    "        # Calculate the binary crossentropy\n",
    "        b_ce=keras.backend.binary_crossentropy(tf.multiply(y_pred, tf.cast(tf.not_equal(y_true, -1), tf.float32)),\n",
    "                                        tf.multiply(y_true, tf.cast(tf.not_equal(y_true, -1), tf.float32)))\n",
    "\n",
    "        # Apply the weights\n",
    "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "        # Return the mean error\n",
    "        return keras.backend.mean(weighted_b_ce)\n",
    "\n",
    "    return weighted_binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom weighted loss for u-one,u-zeros\n",
    "\n",
    "def create_weighted_binary_crossentropy(zero_weight, one_weight):\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "\n",
    "        # Calculate the binary crossentropy\n",
    "        b_ce=keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # Apply the weights\n",
    "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "        # Return the mean error\n",
    "        return keras.backend.mean(weighted_b_ce)\n",
    "\n",
    "    return weighted_binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####citation!!####\n",
    "#### got it form here https://gist.github.com/wassname/ce364fddfc8a025bfab4348cf5de852d ### \n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = keras.backend.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= keras.backend.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = keras.backend.clip(y_pred, keras.backend.epsilon(), 1 - keras.backend.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * keras.backend.log(y_pred) * weights\n",
    "        loss = -keras.backend.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list = input_fn.compute_weights(new_trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{-1.0: 26987, 0.0: 124991, 1.0: 26753},\n",
       " {-1.0: 6468, 0.0: 150701, 1.0: 21562},\n",
       " {-1.0: 22247, 0.0: 144611, 1.0: 11873},\n",
       " {-1.0: 10352, 0.0: 126566, 1.0: 41813},\n",
       " {-1.0: 9391, 0.0: 100397, 1.0: 68943}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if approach == 'u-MultiClass':\n",
    "    loss1 = weighted_categorical_crossentropy([1, round(weight_list[1][0.0]/weight_list[1][1.0]), round(weight_list[1][0.0]/weight_list[1][2.0])]) \n",
    "    loss2 = weighted_categorical_crossentropy([1, round(weight_list[2][0.0]/weight_list[2][1.0]), round(weight_list[2][0.0]/weight_list[2][2.0])]) \n",
    "    loss3 = weighted_categorical_crossentropy([1, round(weight_list[3][0.0]/weight_list[3][1.0]), round(weight_list[3][0.0]/weight_list[3][2.0])]) \n",
    "    loss4 = weighted_categorical_crossentropy([1, round(weight_list[4][0.0]/weight_list[4][1.0]), round(weight_list[4][0.0]/weight_list[4][2.0])]) \n",
    "    loss5 = weighted_categorical_crossentropy([1, round(weight_list[5][0.0]/weight_list[5][1.0]), round(weight_list[5][0.0]/weight_list[5][2.0])]) \n",
    "elif approach == 'u-Ignore':\n",
    "    loss1= create_mask_weighted_binary_crossentropy(1,round(weight_list[0][0.0]/weight_list[0][1.0]))\n",
    "    loss2 = create_mask_weighted_binary_crossentropy(1,round(weight_list[1][0.0]/weight_list[1][1.0]))\n",
    "    loss3 = create_mask_weighted_binary_crossentropy(1,round(weight_list[2][0.0]/weight_list[2][1.0]))\n",
    "    loss4 = create_mask_weighted_binary_crossentropy(1,round(weight_list[3][0.0]/weight_list[3][1.0]))\n",
    "    loss5 = create_mask_weighted_binary_crossentropy(1,round(weight_list[4][0.0]/weight_list[4][1.0]))\n",
    "else:\n",
    "    loss1= create_weighted_binary_crossentropy(1,round(weight_list[0][0.0]/weight_list[0][1.0]))\n",
    "    loss2 = create_weighted_binary_crossentropy(1,round(weight_list[1][0.0]/weight_list[1][1.0]))\n",
    "    loss3 = create_weighted_binary_crossentropy(1,round(weight_list[2][0.0]/weight_list[2][1.0]))\n",
    "    loss4 = create_weighted_binary_crossentropy(1,round(weight_list[3][0.0]/weight_list[3][1.0]))\n",
    "    loss5 = create_weighted_binary_crossentropy(1,round(weight_list[4][0.0]/weight_list[4][1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names=['Atelectasis',\n",
    " 'Cardiomegaly',\n",
    " 'Cosolidation',\n",
    " 'Edema',\n",
    " 'PleuralEffusion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Model <br>\n",
    "- Can be configured to use DenseNet-w ImageNet weights, EfficientNetB3-w ImageNet weights, DenseNet121-w Chexnet weights <br> \n",
    "- Also can be configured to do transfer learning or fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE=(image_size,image_size,3)\n",
    "\n",
    "if chexnet==True:\n",
    "    base_model=keras.applications.DenseNet121(weights=None, include_top=False, input_shape=IMG_SHAPE)\n",
    "    base_model.load_weights(\"ChexNet_weights_notop.h5\")\n",
    "elif efficientnet==True:\n",
    "    base_model=efn.EfficientNetB3(weights='imagenet', include_top=False,input_shape=IMG_SHAPE)\n",
    "else:\n",
    "    base_model=keras.applications.DenseNet121(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)\n",
    "\n",
    "\n",
    "x = base_model.output\n",
    "x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256,activation='relu',name=\"fc\") (x)\n",
    "x = keras.layers.Dropout(0.5,name=\"dropout_final\")(x)\n",
    "if approach == 'u-MultiClass':\n",
    "    num_outputs = 3\n",
    "    activation = 'softmax'\n",
    "else:\n",
    "    num_outputs = 1\n",
    "    activation= 'sigmoid'\n",
    "### one output for each pathology    \n",
    "out1 = keras.layers.Dense(num_outputs,activation=activation,name=col_names[0]) (x)\n",
    "out2 = keras.layers.Dense(num_outputs,activation=activation,name=col_names[1]) (x)\n",
    "out3 = keras.layers.Dense(num_outputs,activation=activation,name=col_names[2]) (x)\n",
    "out4 = keras.layers.Dense(num_outputs,activation=activation,name=col_names[3]) (x)\n",
    "out5 = keras.layers.Dense(num_outputs,activation=activation,name=col_names[4]) (x)\n",
    "\n",
    "\n",
    "def get_layer_index(layer_name):\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if layer_name in layer.name:\n",
    "            break;\n",
    "    return i+1\n",
    "\n",
    "adam=tf.keras.optimizers.Adam(lr=lr)\n",
    "sgd =tf.keras.optimizers.SGD(lr = lr, momentum=0.8,nesterov=False) \n",
    "\n",
    "if finetune:\n",
    "    model = Model(base_model.input,[out1, out2, out3, out4, out5])\n",
    "    optimizer = sgd\n",
    "else:\n",
    "    base_model.trainable=False\n",
    "    model = Model(base_model.input,[out1, out2, out3, out4, out5])\n",
    "    optimizer = adam\n",
    "\n",
    "\n",
    "if approach =='u-Ignore':\n",
    "    accuracy = binary_accuracy\n",
    "else:\n",
    "    accuracy = keras.metrics.categorical_accuracy\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss= {col_names[0]:loss1,\n",
    "                   col_names[1]:loss2,\n",
    "                   col_names[2]:loss3,\n",
    "                   col_names[3]:loss4,\n",
    "                   col_names[4]:loss5},\n",
    "              loss_weights = {\n",
    "                  col_names[0]:0.51,\n",
    "                   col_names[1]:0.44,\n",
    "                   col_names[2]:0.79,\n",
    "                   col_names[3]:0.39,\n",
    "                   col_names[4]:0.35},\n",
    "             metrics=['accuracy',accuracy])\n",
    "\n",
    "if finetune:\n",
    "    for layer in model.layers[:get_layer_index('conv5_block13_2_conv')]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_checkpoint==True:\n",
    "    model.load_weights(os.path.join('output',checkpoint_path, 'cp.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_x) // batch_size\n",
    "validation_steps = len(validation_x) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##make directory for this approach \n",
    "model_direct = os.path.join('output',model_dir)\n",
    "os.makedirs(model_direct,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##make checkpoint path\n",
    "checkpoint_path = os.path.join('output',model_dir,checkpoint_path)\n",
    "os.makedirs(checkpoint_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##checkpoint file name\n",
    "checkpoint_file = os.path.join(checkpoint_path,\"cp.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint callback\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(checkpoint_file,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=True,\n",
    "                                                 mode='auto',\n",
    "                                                 monitor='val_loss')\n",
    "earlystopper=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step decay** not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def step_decay(epoch):\n",
    "#     init_lrate =0.001 \n",
    "#     drop = 0.5 \n",
    "#     epochs_drop = 2.0\n",
    "#     lrate = init_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
    "#     return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 11170 steps, validate for 2792 steps\n",
      "Epoch 1/20\n",
      "  268/11170 [..............................] - ETA: 18:52 - loss: 27.4661 - Atelectasis_loss: 10.8622 - Cardiomegaly_loss: 12.6209 - Cosolidation_loss: 12.6581 - Edema_loss: 10.7372 - PleuralEffusion_loss: 6.2451 - Atelectasis_accuracy: 0.2397 - Atelectasis_binary_accuracy: 0.2829 - Cardiomegaly_accuracy: 0.3282 - Cardiomegaly_binary_accuracy: 0.3394 - Cosolidation_accuracy: 0.5864 - Cosolidation_binary_accuracy: 0.6700 - Edema_accuracy: 0.5562 - Edema_binary_accuracy: 0.5875 - PleuralEffusion_accuracy: 0.5468 - PleuralEffusion_binary_accuracy: 0.5769WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,Atelectasis_loss,Cardiomegaly_loss,Cosolidation_loss,Edema_loss,PleuralEffusion_loss,Atelectasis_accuracy,Atelectasis_binary_accuracy,Cardiomegaly_accuracy,Cardiomegaly_binary_accuracy,Cosolidation_accuracy,Cosolidation_binary_accuracy,Edema_accuracy,Edema_binary_accuracy,PleuralEffusion_accuracy,PleuralEffusion_binary_accuracy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d24b227f1ac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     callbacks=[checkpointer,earlystopper])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mbatch_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             batch_end=step * batch_size + current_batch_size)\n\u001b[0;32m--> 173\u001b[0;31m       \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m       \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mmake_logs\u001b[0;34m(model, logs, outputs, mode, prefix)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   \u001b[0;34m\"\"\"Computes logs for sending to `on_batch_end` methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0mmetric_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmetric_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mmetrics_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;31m# Add all metric names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mmetrics_names\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    412\u001b[0m       \u001b[0mmetrics\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_metric_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_metrics_from_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_get_metrics_from_layers\u001b[0;34m(layers)\u001b[0m\n\u001b[1;32m   3241\u001b[0m       \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_metrics_from_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3242\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3243\u001b[0;31m       \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3244\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_subclass_implementers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_gather_children_attribute\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m   2333\u001b[0m           self._layers)\n\u001b[1;32m   2334\u001b[0m       return list(\n\u001b[0;32m-> 2335\u001b[0;31m           itertools.chain.from_iterable(\n\u001b[0m\u001b[1;32m   2336\u001b[0m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_input,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=epoches,\n",
    "                    validation_data=eval_input,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=[checkpointer,earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path= os.path.join('output',model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x,test_y=input_fn.process_testing_data(file_path=test)\n",
    "test_input= input_fn.input_fn_multi_output(False,test_x, None,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= load_model(model_path, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####needs to do if loading from history\n",
    "\n",
    "# if approach =='u-Ignore':\n",
    "#     model= load_model(model_path, custom_objects={'mask_binary_crossentropy':mask_binary_crossentropy})\n",
    "# else:\n",
    "#     model= load_model(model_path, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_steps = len(test_x) // batch_size\n",
    "result=model.predict(test_input,steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Pred_Atelectasis\",\"Pred_Cardiomegaly\",\"Pred_Consolidation\",\"Pred_Edema\",\"Pred_PleuralEffusion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save test result csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultdf= pd.DataFrame()\n",
    "for i in range(len(result)):\n",
    "    resultdf[columns[i]]=result[i].flatten()\n",
    "resultdf=resultdf.join(test_y['Atelectasis'])\n",
    "resultdf=resultdf.join(test_y['Cardiomegaly'])\n",
    "resultdf=resultdf.join(test_y['Consolidation'])\n",
    "resultdf=resultdf.join(test_y['Edema'])\n",
    "resultdf=resultdf.join(test_y['Pleural Effusion'])\n",
    "resultdf['image_path']=test_x[:len(resultdf)]\n",
    "save_name = 'resources/'+'results_'+'weighted_'+'5_'+'Chex'+'_ImgAUg'+model_dir+'.csv'\n",
    "resultdf.to_csv(save_name)\n",
    "resultdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names=['Atelectasis',\n",
    "'Cardiomegaly',\n",
    "'Cosolidation',\n",
    "'Edema',\n",
    "'PleuralEffusion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot AUROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_auroc_curve(resultdf, \"uIgnore_imgAug_weighted_5_Chex_auroc.png\",approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for i in range(5):\n",
    "    y_pred.append(( model.predict(test_input)[i] >0.4).astype(int))\n",
    "\n",
    "result_hard_labels_df= pd.DataFrame()\n",
    "for i in range(len(result)):\n",
    "    result_hard_labels_df[columns[i]]=y_pred[i].flatten()\n",
    "    \n",
    "result_hard_labels_df=result_hard_labels_df.join(test_y['Atelectasis'])\n",
    "result_hard_labels_df=result_hard_labels_df.join(test_y['Cardiomegaly'])\n",
    "result_hard_labels_df=result_hard_labels_df.join(test_y['Consolidation'])\n",
    "result_hard_labels_df=result_hard_labels_df.join(test_y['Edema'])\n",
    "result_hard_labels_df=result_hard_labels_df.join(test_y['Pleural Effusion'])\n",
    "#result_hard_labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot PRAUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Confusion Matrices for all the labels\n",
    "\n",
    "for i in range(len(y_pred)):    \n",
    "    \n",
    "    plot_confusion_matrix_custom(result_hard_labels_df.loc[:,result_hard_labels_df.columns[i+len(y_pred)]],result_hard_labels_df.loc[:,result_hard_labels_df.columns[i]],result_hard_labels_df.columns[i+len(y_pred)],approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(resultdf, \"uIgnore_imgAUg_weighted_5_Chex_precision_recall.png\",approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 02_Train_Test.ipynb to html\n",
      "[NbConvertApp] Writing 369484 bytes to 02_Train_Test.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html 02_Train_Test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
